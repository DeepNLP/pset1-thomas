{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import scipy.misc as sp\n",
    "\n",
    "from q1_softmax import softmax\n",
    "from q2_gradcheck import gradcheck_naive\n",
    "from q2_sigmoid import sigmoid, sigmoid_grad\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    # Implement a function that normalizes each row of a matrix to have unit length\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    row_norm = np.sqrt(np.sum(x * x, axis=1, keepdims=True))\n",
    "    x = x / row_norm\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return x\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print \"Testing normalizeRows...\"\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]])) \n",
    "    # the result should be [[0.6, 0.8], [0.4472, 0.8944]]\n",
    "    print x\n",
    "    assert (x.all() == np.array([[0.6, 0.8], [0.4472, 0.8944]]).all())\n",
    "    print \"Ok normalizeRows !\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, grad = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        ### try modifying x[ix] with h defined above to compute numerical gradients\n",
    "        ### make sure you call random.setstate(rndstate) before calling f(x) each time, this will make it \n",
    "        ### possible to test cost functions with built in randomness later\n",
    "        ### YOUR CODE HERE:\n",
    "        epsilon = np.zeros(x.shape)\n",
    "        epsilon[ix] = h\n",
    "\n",
    "        random.setstate(rndstate)\n",
    "        f1 = f(x + epsilon)\n",
    "\n",
    "        random.setstate(rndstate)\n",
    "        f2 = f(x - epsilon)\n",
    "        \n",
    "        numgrad = (f1[0] - f2[0])/2/h\n",
    "        ### END YOUR CODE\n",
    "\n",
    "        # Compare gradients\n",
    "        reldiff = abs(numgrad - grad[ix]) / max(1, abs(numgrad), abs(grad[ix]))\n",
    "        if reldiff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (grad[ix], numgrad)\n",
    "            return\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "        \n",
    "    return cost, grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_word2vec():\n",
    "    # Interface to the dataset for negative sampling\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] \\\n",
    "           for i in xrange(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "    print \"\\n=== Results ===\"\n",
    "    print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    # Interface to the dataset for negative sampling\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] \\\n",
    "           for i in xrange(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: {'a': 0, 'c': 2, 'b': 1, 'e': 4, 'd': 3}\n"
     ]
    }
   ],
   "source": [
    "print \"hello: \" + str(dummy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    \n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, assuming the softmax prediction function and cross      \n",
    "    # entropy loss.                                                   \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - predicted: numpy ndarray, predicted word vector (\\hat{v} in \n",
    "    #   the written component or \\hat{r} in an earlier version)\n",
    "    # - target: integer, the index of the target word               \n",
    "    # - outputVectors: \"output\" vectors (as rows) for all tokens     \n",
    "    # - dataset: needed for negative sampling, unused here.         \n",
    "    \n",
    "    # Outputs:                                                        \n",
    "    # - cost: cross entropy cost for the softmax word prediction    \n",
    "    # - gradPred: the gradient with respect to the predicted word   \n",
    "    #        vector                                                \n",
    "    # - grad: the gradient with respect to all the other word        \n",
    "    #        vectors                                               \n",
    "    \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!                                                  \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # Compute softmax matrix\n",
    "    prod = np.matmul(outputVectors, predicted)\n",
    "    softmax_matrix = softmax(prod)\n",
    "    #sumexp = np.sum(np.exp(prod)) => overflow\n",
    "\n",
    "    # Compute cost\n",
    "    cost = - np.log(softmax_matrix)[target]\n",
    "\n",
    " \t# Compute gradients\n",
    "    negpart = np.matmul(softmax_matrix, outputVectors)\n",
    "#    print \"softmax_matrix: \" + str(softmax_matrix)\n",
    "#    print \"softmax_matrix shape: \" + str(softmax_matrix.shape)\n",
    "#    print \"outputVectors: \" + str(outputVectors)\n",
    "#    print \"outputVectors shape: \" + str(outputVectors.shape)\n",
    "#    print \"negpart: \" + str(negpart)\n",
    "    gradPred = - outputVectors[target] + negpart\n",
    "\n",
    "    grad = np.outer(softmax_matrix, predicted)\n",
    "    grad[target] -= predicted\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, \n",
    "    K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "\n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, using the negative sampling technique. K is the sample  \n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  \n",
    "    # a random word index. \n",
    "    # \n",
    "    # Note: See test_word2vec below for dataset's initialization.\n",
    "    #                                       \n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "#    print \"predicted: \" + str(predicted)\n",
    "#    print \"target: \" + str(target)\n",
    "#    print \"outputVectors\" + str(outputVectors)\n",
    "    # Sample K negative samples\n",
    "    neg_samples = np.zeros((K,predicted.size))\n",
    "    neg_sample_idx = []\n",
    "    for k in range(K):\n",
    "        idx = dataset.sampleTokenIdx()\n",
    "        while idx == target:\n",
    "            idx = dataset.sampleTokenIdx()\n",
    "        neg_sample_idx.append(idx)\n",
    "    \tneg_samples[k] = outputVectors[idx]\n",
    "#    print \"neg_sample_idx\" + str(neg_sample_idx)\n",
    "#    print \"neg_samples: \"+ str(neg_samples)\n",
    "\n",
    "    prod = np.matmul(neg_samples, predicted)\n",
    "#    print \"prod: \" + str(prod)\n",
    "    # Compute cost\n",
    "    cost = - np.log(sigmoid(np.dot(outputVectors[target], predicted))) - np.sum(np.log(sigmoid(-prod)))\n",
    "#    print \"cost: \" + str(cost)\n",
    "\n",
    " \t# Compute gradients\n",
    "    sigm = sigmoid(-np.dot(outputVectors[target], predicted))\n",
    "    \n",
    "    negpart = np.zeros(predicted.shape)\n",
    "    for k in range(10):\n",
    "        negpart += sigmoid(prod)[k] * neg_samples[k]\n",
    "#    print \"negative samples:\" + str(neg_samples)\n",
    "#    print \"negative samples shape:\" + str(neg_samples.shape)\n",
    "#    print \"sigmoid(prod): \" + str(sigmoid(prod))\n",
    "#    print \"sigmoid(prod) shape: \" + str(sigmoid(prod).shape)\n",
    "#    print \"negmat: \" + str(negmat)\n",
    "#    print \"negmat shape: \" + str(negmat.shape)\n",
    "#\tnegpart = np.sum(negmat, axis = 0)\n",
    "#    print \"negpart: \" + str(negpart)\n",
    "    gradPred = - sigm * outputVectors[target] + negpart\n",
    "#    print \"gradPred: \" + str(gradPred)\n",
    "\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    for i in range(K):\n",
    "        grad[neg_sample_idx[i]] += sigmoid(prod[i]) * predicted\n",
    "    grad[target] -= sigm * predicted\n",
    "#    print \"grad: \" + str(grad)\n",
    "\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the skip-gram model in this function.\n",
    "\n",
    "    # Inputs:                                                         \n",
    "    # - currrentWord: a string of the current center word           \n",
    "    # - C: integer, context size                                    \n",
    "    # - contextWords: list of no more than 2*C strings, the context words                                               \n",
    "    # - tokens: a dictionary that maps words to their indices in    \n",
    "    #      the word vector list                                \n",
    "    # - inputVectors: \"input\" word vectors (as rows) for all tokens           \n",
    "    # - outputVectors: \"output\" word vectors (as rows) for all tokens         \n",
    "    # - word2vecCostAndGradient: the cost and gradient function for \n",
    "    #      a prediction vector given the target word vectors,  \n",
    "    #      could be one of the two cost functions you          \n",
    "    #      implemented above\n",
    "\n",
    "    # Outputs:                                                        \n",
    "    # - cost: the cost function value for the skip-gram model       \n",
    "    # - grad: the gradient with respect to the word vectors         \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    pred_index = tokens[currentWord]\n",
    "    cost = 0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    for m in range(2*C):\n",
    "    \ttarget_index = tokens[contextWords[m]]\n",
    "    \tsingle_cost, gradPred, grad = word2vecCostAndGradient(inputVectors[pred_index], target_index, outputVectors, dataset)\n",
    "#        print \"cost: \" + str(single_cost)\n",
    "#        print \"gradPred: \" + str(gradPred)\n",
    "#        print \"grad: \" + str(grad)\n",
    "    \tcost += single_cost\n",
    "    \tgradIn[pred_index] += gradPred\n",
    "    \tgradOut += grad\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 27.1730762768\n",
      "gin: [[ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]\n",
      " [-5.78657829  1.02888708 -3.68467947]\n",
      " [ 0.          0.          0.        ]\n",
      " [ 0.          0.          0.        ]]\n",
      "gout: [[-1.07736921  0.49437484  3.76059273]\n",
      " [-1.10088306  0.50516469  3.84266862]\n",
      " [-0.97879559  0.44914214  3.41651825]\n",
      " [-1.15876239  0.53172391  4.04469832]\n",
      " [-0.09848948  0.04519409  0.34378076]]\n"
     ]
    }
   ],
   "source": [
    "#word2vec_sgd_wrapper(skipgram, dummy_tokens, dummy_vectors, dataset, 5)\n",
    "#word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient)\n",
    "\n",
    "batchsize = 50\n",
    "cost = 0.0\n",
    "grad = np.zeros(dummy_vectors.shape)\n",
    "N = dummy_vectors.shape[0]\n",
    "inputVectors = dummy_vectors[:N/2,:]\n",
    "outputVectors = dummy_vectors[N/2:,:]\n",
    "C1 = random.randint(1,5)\n",
    "centerword, context = dataset.getRandomContext(C1)\n",
    "c, gin, gout = skipgram(centerword, C1, context, dummy_tokens, inputVectors, outputVectors, dataset, negSamplingCostAndGradient)\n",
    "print \"c: \" +str(c)\n",
    "print \"gin: \" + str(gin)\n",
    "print\"gout: \"+str(gout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3: (3,)\n",
      "-2: (10, 3)\n",
      "-1: (10,)\n",
      "0: (3,)\n",
      "1: -1.65231213606\n",
      "2: [-1.65231214  1.83785522 -0.9535864 ]\n"
     ]
    }
   ],
   "source": [
    "predicted = np.array([-0.56713774, -0.27178229, -0.77748902])\n",
    "neg_samples= np.array([[ 0.18289107,  0.76098587, -0.62245591],\n",
    " [-0.6831809,  -0.04200519,  0.72904007],\n",
    " [-0.6831809,  -0.04200519,  0.72904007],\n",
    " [ 0.18289107,  0.76098587, -0.62245591],\n",
    " [-0.61517874,  0.5147624,  -0.59713884],\n",
    " [-0.52629529, -0.78190408,  0.33412466],\n",
    " [-0.61517874,  0.5147624,  -0.59713884],\n",
    " [ 0.18289107,  0.76098587, -0.62245591],\n",
    " [ 0.18289107,  0.76098587, -0.62245591],\n",
    " [-0.6831809,  -0.04200519,  0.72904007]])\n",
    "print \"-3: \"+str(predicted.shape)\n",
    "print \"-2: \"+str(neg_samples.shape)\n",
    "prod = np.matmul(neg_samples, predicted)\n",
    "print \"-1: \"+str(prod.shape)\n",
    "negmat = np.matmul(neg_samples.T,sigmoid(prod))\n",
    "print \"0: \"+str(negmat.shape)\n",
    "print \"1: \"+str(negmat[0])\n",
    "vec_sum = np.zeros(predicted.shape)\n",
    "for k in range(10):\n",
    "    vec_sum += sigmoid(prod)[k] * neg_samples[k]\n",
    "print \"2: \"+str(vec_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 'cbow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-42b34e710449>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-6afa92fc2624>\u001b[0m in \u001b[0;36mtest_word2vec\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegSamplingCostAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\n==== Gradient check for CBOW      ====\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegSamplingCostAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-cbd85826c0fb>\u001b[0m in \u001b[0;36mgradcheck_naive\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mrndstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrndstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mfx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Evaluate function value at original point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-6afa92fc2624>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(vec)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskipgram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegSamplingCostAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"\\n==== Gradient check for CBOW      ====\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mgradcheck_naive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mword2vec_sgd_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegSamplingCostAndGradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'cbow' is not defined"
     ]
    }
   ],
   "source": [
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
